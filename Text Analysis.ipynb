{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1BCTWINYd4l8cZBvH65d5RJbBxZbGvEZ7","authorship_tag":"ABX9TyNG7YmN3oWY8rhwK4JKwaPz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Text Analysis"],"metadata":{"id":"dMkSxaqpjCXA"}},{"cell_type":"code","source":["!pip install contractions\n","!pip install syllables\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"],"metadata":{"id":"tIff88w2tssO","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","import re\n","import string\n","import contractions\n","import syllables\n","import pandas as pd"],"metadata":{"id":"qA_P669S8DU7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/Text Analysis/Content_file.csv', index_col='Unnamed: 0')\n","df.head()"],"metadata":{"id":"N5qUtzcH0IPG","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['Text_Content'].isna().value_counts()"],"metadata":{"id":"brZRcnAzeVp4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Removing the rows whose URL are not valid\n","\n","df.dropna(inplace=True)\n","df.columns = df.columns.str.lower()"],"metadata":{"id":"2zoX2gEOKyNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sentimental Analysis"],"metadata":{"id":"U_OW4jrwPWSp"}},{"cell_type":"markdown","source":["### Cleaning using Stop Words Lists"],"metadata":{"id":"X_IXzpdRPcn0"}},{"cell_type":"code","source":["# Instantiating WordNetLemmatizer\n","\n","lem = WordNetLemmatizer()"],"metadata":{"id":"zgcuYJfv4HPV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aC-lZxW265et"},"outputs":[],"source":["# Creating a list of StopWords \n","path = '/content/drive/MyDrive/Text Analysis/StopWords/'\n","stop_words = []\n","\n","for fl in os.listdir(path):\n","  with open(path+fl, 'r', encoding='cp1252') as f:\n","    for w in f:\n","      stop_words.append(w.split()[0])\n","\n","len(stop_words)"]},{"cell_type":"code","source":["for x in stopwords.words('english'):\n","  if x not in stop_words:\n","    stop_words.append(x)\n","\n","len(stop_words)"],"metadata":{"id":"9Sxk2CB2dwK8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Converting the Stopwords into lowercase words\n","stop_words = [w.lower() for w in stop_words]\n","len(stop_words)"],"metadata":{"id":"a2_fRtO75GT2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to pre-process the text data\n","def preprocess(s):\n","  s = s.lower()\n","  s = contractions.fix(s)\n","  s = re.sub('[^a-zA-Z]+', ' ', s).strip()\n","  tokens = word_tokenize(s)\n","  words = [token for token in tokens if token not in stop_words]\n","  lemma = [lem.lemmatize(word) for word in words]\n","  return lemma"],"metadata":{"id":"APSdhewJ6cV3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply the function on extracted textual data\n","preprocessed_data = [preprocess(i) for i in df['text_content']]\n","df['preprocessed_text'] = preprocessed_data"],"metadata":{"id":"Ua7Ni7Bm8CWQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creating a dictionary of Positive and Negative words"],"metadata":{"id":"r0Cke0haQRfI"}},{"cell_type":"code","source":["# Reading the text files with Negative and Positive words\n","with open('/content/drive/MyDrive/Text Analysis/Master Dictionary/negative-words.txt', 'r', encoding='cp1252') as f:\n","  neg_words = f.read().split()\n","\n","with open('/content/drive/MyDrive/Text Analysis/Master Dictionary/positive-words.txt', 'r', encoding='cp1252') as f:\n","  pos_words = f.read().split()\n","\n","len(pos_words), len(neg_words)"],"metadata":{"id":"XmJADp7Xwka-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a dictionary of Positive and Negative words\n","pos_neg_words = {'Positive': pos_words,\n","                 'Negtive': neg_words}"],"metadata":{"id":"PIPrvWZO3hIP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Extracting Derived variables"],"metadata":{"id":"BPkjMJd3Q-1k"}},{"cell_type":"markdown","source":["#### Positive and Negtive Score"],"metadata":{"id":"d7bu6w8eRun6"}},{"cell_type":"code","source":["pos_score = []\n","neg_score = []\n","for i in preprocessed_data:\n","  pos = []\n","  neg = []\n","  for j in i:\n","    if j in pos_neg_words['Positive']:\n","      pos.append(j)\n","    if j in pos_neg_words['Negtive']:\n","      neg.append(j)\n","  pos_score.append(len(pos))\n","  neg_score.append(len(neg))"],"metadata":{"id":"ceOfgpO9R2Ba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adding Positive and Negtive columns to the dataframe\n","df['positive'] = pos_score\n","df['negative'] = neg_score"],"metadata":{"id":"X0wTUJCFc1WR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Polarity Score"],"metadata":{"id":"mO3bos5GYPW5"}},{"cell_type":"code","source":["df['polarity'] = round((df['positive'] - df['negative'])/(df['positive'] + df['negative'] + 0.000001), 2)"],"metadata":{"id":"mvcQhZBZYalh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Subjective Score"],"metadata":{"id":"csqNpG0odubQ"}},{"cell_type":"code","source":["df['num_words'] = [len(x) for x in df['preprocessed_text']]"],"metadata":{"id":"JTf3wy54eXqQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['subject'] = round((df['positive']+df['negative'])/(df['num_words'] + 0.000001), 2)"],"metadata":{"id":"rFwbOAbadyN9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.sample(5)"],"metadata":{"id":"si3n4YJzQuby"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##\tAnalysis of Readability"],"metadata":{"id":"2WcvCZlKhrz0"}},{"cell_type":"markdown","source":["#### Average Sentence Length"],"metadata":{"id":"G9piNeB1NY4P"}},{"cell_type":"code","source":["df['num_sent'] = [len(sent_tokenize(sen)) for sen in df['text_content']]"],"metadata":{"id":"hRrmZwNyi2s_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['avg_sen_len'] = round(df['num_words']/df['num_sent'], 2)\n","df.sample(5)"],"metadata":{"id":"0MLvJus63g9b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Percentage of Complex Words"],"metadata":{"id":"Pt9kBaEhNeWq"}},{"cell_type":"code","source":["def complex_words(sen):\n","  complexity = set()\n","  for w in sen:\n","    if syllables.estimate(w) > 2 and w[-2:] != 'ed' and w[-2:] != 'es':\n","      complexity.add(w)\n","  return len(complexity)\n"],"metadata":{"id":"EjCImMNmlTXh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['complex_words'] = [complex_words(w) for w in df['preprocessed_text']]"],"metadata":{"id":"kn_5cVb7nXrw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['%age_complex_words'] = round(df['complex_words']/df['num_words'], 2)"],"metadata":{"id":"R_PWPwJsnlOF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Fog Index"],"metadata":{"id":"gD1bI3tZOOl_"}},{"cell_type":"code","source":["df['fog_index'] = round( 0.4 * (df['avg_sen_len'] + df['%age_complex_words']), 2)"],"metadata":{"id":"N382YaJwfYOR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.sample(3)"],"metadata":{"id":"96XOrq0sMXW-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Average Number of Words Per Sentence"],"metadata":{"id":"4MtVnZ50mmHz"}},{"cell_type":"code","source":["df['avg_words_per_sen'] = round(df['num_words']/df['num_sent'], 2)"],"metadata":{"id":"1Fr9Qvc-mmFc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.sample(3)"],"metadata":{"id":"T1ViqZAvOq_N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Complex Word Count and Word Count\n","\n","These is already calculated."],"metadata":{"id":"NsD9IlPZ0Ppj"}},{"cell_type":"code","source":["df.sample(5)"],"metadata":{"id":"ZpiXx4JX0dzl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Syllable Count Per Word"],"metadata":{"id":"nnSPm2ihmmCw"}},{"cell_type":"code","source":["df['syl_count'] = [syllables.estimate(' '.join(w)) for w in df['preprocessed_text']]"],"metadata":{"id":"MW8UldFCmcgt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['syl_per_word'] = round(df['syl_count']/df['num_words'], 2)"],"metadata":{"id":"YjrsGvfEoInz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.sample(3)"],"metadata":{"id":"pR7SswDvoXt7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Personal Pronouns"],"metadata":{"id":"yf6cggjxoZGr"}},{"cell_type":"code","source":["def personal_pronouns(text):\n","  pronoun_sample = re.compile(r'\\b(I|we|my|ours|(?-i:us))\\b', re.I)\n","  pronouns = pronoun_sample.findall(text)\n","  return len(pronouns)"],"metadata":{"id":"4UYeuHjao3j_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['personal_pronouns'] = [personal_pronouns(sen) for sen in df['text_content']]"],"metadata":{"id":"n_oI9f0Opm7b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.sample(3)"],"metadata":{"id":"aU13QELEqII9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Average Word Length"],"metadata":{"id":"R3yJk6MKqKWi"}},{"cell_type":"code","source":["def text_len(text):\n","  filtered = ''.join(filter(lambda x: x not in string.punctuation, text))\n","  words = [word for word in filtered.split() if word]\n","  ch_len = 0\n","  for w in words:\n","    ch_len += len(w)\n","  avg = ch_len/len(words)\n","  return avg"],"metadata":{"id":"tP19cFXCqU-I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['avg_word_len'] = [text_len(text) for text in df['text_content']]"],"metadata":{"id":"S3WjOWhDsCYe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.sample(3)"],"metadata":{"id":"wbEQ5YNkzvrr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creating Output Structure"],"metadata":{"id":"L6IXzteezyEZ"}},{"cell_type":"code","source":["df = df[['url_id', 'url', 'positive', 'negative', 'polarity', 'subject', 'avg_sen_len', '%age_complex_words', 'fog_index', 'avg_words_per_sen', 'complex_words', 'num_words', 'syl_per_word', 'personal_pronouns', 'avg_word_len']]"],"metadata":{"id":"z0Tk1G8O0EEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.columns = ['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']"],"metadata":{"id":"HcxiHR9Z0zqd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"DbTd4UrU8-zB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_csv('/content/drive/MyDrive/Text Analysis/Output Data.csv', index=False)"],"metadata":{"id":"EKND990f9GDB"},"execution_count":null,"outputs":[]}]}